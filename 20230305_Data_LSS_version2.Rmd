---
title: "LSS"
author: "Elias Mayer"
date: "2022-10-29"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r libs, echo=FALSE, warning=FALSE, include=FALSE}

library("ggpmisc")

library("tidyverse")
library("tidytext")
library("tokenizers")
library("tidymodels")

library("purrr")


library ("plyr")
library("stopwords")
library("readr")
library("quanteda")
library("quanteda.textstats")
library("quanteda.textplots")

library("quanteda")
library("LSX") 

# For stock symbols
library("TTR")

# English lexicons not sentiment
library("qdapDictionaries")

# Head tail
library("psych")

# imputes if needed
library("imputeTS")
library("tseries")
library("forecast")
library("urca")
library("outliers") #containing function outlier

# font_import() this is required for chosen clean theme 

#library(extrafont) 

#loadfonts(device = "win")

#extrafont::font_import()

#detach("package:hrbrthemes", unload=TRUE)

library("ragg")
library("hrbrthemes")

hrbrthemes::import_roboto_condensed()

library("rlist")
library("DescTools")
library("newsmap")

library("tsibble")

# for ts conversions
library("tsbox")

# variable lag granger
library("VLTimeCausality")

# TS and plotting
library("zoo")
library("cowplot")

# Clean console output
library("insight")
library("lubridate")
library("timetk")

library("forecast")

# variable lag granger
library("VLTimeCausality")
library("PerformanceAnalytics")

library("vars")

```

##  Load data and preperation

```{r TrainLSS, echo=FALSE}


# Adjust path to Folder Struct 

basePathC <- 'C:/Users/elias/OneDrive/Desktop/Meme_Thesis/3 Data/Cleaned_relevant_data'

# Paths for Ticker data

basepath <- "C:/Users/elias/OneDrive/Desktop/Meme_Thesis/3 Data/Cleaned_relevant_data/Ticker specific/"
  
tickerBasePath <- "C:/Users/elias/OneDrive/Desktop/Meme_Thesis/3 Data/Stock_data_WF/"


# Combiner for graph 

# Source: https://stackoverflow.com/questions/29864318/combination-of-named-vectors 

Combiner <- function(vec1, vec2, vecOut = TRUE) {
  temp <- unique(rbind(data.frame(as.table(vec1)),
                       data.frame(as.table(vec2))))
  if (isTRUE(vecOut)) setNames(temp$Freq, temp$Var1)
  else temp
}

#Train the LSS with the whole corpus available (training) of all OTC tickers to pick up as much language wise as possible 

whole_gen_cmts_language_df <-  read.csv(paste(basePathC, "/reddit_comments/general_comments.csv", sep = ""))

whole_ass_cmts_language_df <-  read.csv(paste(basePathC, "/reddit_comments/associated_comments.csv", sep = ""))

whole_dis_cmts_language_df <-  read.csv(paste(basePathC, "/discord_comments/discord_comments.csv", sep = ""))


# Time split sufficient 

whole_gen_cmts_language_df_cl <- whole_gen_cmts_language_df %>% 
  separate(col=link_id,sep="_",into = c("ID.prefix","link_id_body"))

whole_dis_cmts_language_df_cl <- whole_dis_cmts_language_df %>% 
  dplyr::rename(ID = AuthorID) %>% mutate(Score=1, parentID="Null", ID.prefix = "Null", link_id_body = "Null")

# Append all comments into one data frame (general ,associated and discord)

bind_cmts <- rbind(whole_gen_cmts_language_df_cl,whole_ass_cmts_language_df,whole_dis_cmts_language_df_cl) %>% 
  mutate(DateX = as.Date(Date)) %>% dplyr::select(-Date) %>% dplyr::rename(Date = DateX) %>% distinct()


# Create a corpus combined

corp <- corpus(bind_cmts, text_field = "Body")

summary(docvars(corp))

cutofDateTraining <- "2020-03-01"

sob <- corpus_subset(corp, Date < as.Date(cutofDateTraining))  # Sub-setting for training data 

sum_tok <- summary(sob, n = Inf)

# Preperation, Source: https://tutorials.quanteda.io/machine-learning/lss/

ggplot(data = sum_tok, aes(x = Date, y = Sentences)) +
  geom_area( fill="#69b3a2", alpha=0.4) +
  geom_line(color="#69b3a2", size=0.02) +
  coord_cartesian(ylim=c(0,50))  +  
  theme_ipsum(grid="Y", base_family  = "Roboto Condensed", plot_title_size = 12) 

toks_sent <- sob %>% 
  corpus_reshape("sentence") %>% 
  tokens(remove_punct = TRUE) %>% 
  tokens_remove(stopwords("en"), padding = TRUE)

dfmt_sent <- toks_sent %>% 
  dfm(remove_padding = TRUE) %>%
  dfm_select("^\\p{L}+$", valuetype = "regex", min_nchar = 2) %>% 
  dfm_trim(min_termfreq = 10)

textplot_wordcloud(dfmt_sent, min_count = 6, random_order = FALSE, rotation = 0.25, max_words = 100)


```



```{r SeedwordsAFE, echo=FALSE}

# Function which filters and gives the AFE stat for given data 

vectorCoOccurence <- function(DATA) {

    dat_tst <- DATA
    
    corb <- quanteda::corpus(dat_tst, text_field = "text") %>% tokens(remove_punct = TRUE)  %>% 
      dfm(remove_padding = TRUE) 
    
    dfm_labels <- dat_tst$category %>% tokens(remove_punct = TRUE) %>%  dfm(remove_padding = TRUE) 
    
    # create co-occurrence vectors
    afe_stat <- newsmap::afe(corb, dfm_labels, smooth = 1)

    return(afe_stat)
}

```

# Classify for AFE

```{r SeedwordsAFEAddition, echo=FALSE, warning=FALSE}

# Potential seed words from frequency selected after human double check

positive_nw_iter <- c( "good", "bought","right","pump",
                       "nice","holding","great", "gain", "buy", 
                       "upvote", "strong", "super", "call", 
                       "moon", "love","positive","win")

negative_nw_iter <- c( "bad","short", "sold","wrong","dump",
                       "shit","selling","concerns", "drop","sell", 
                       "downvote", "spam", "fuck", "scam",
                       "put","panic","negative", "loss")

pnsSeedPotential<- c(positive_nw_iter,negative_nw_iter)

# Array for results 

arrayResults <- array(dim = c(length(pnsSeedPotential),2)) 

colnames(arrayResults) <- c("+ Seedword", "AFE")

c = 1

# Start words

wordName = c("positive","negative")

words <- list()

words$positive <-  c("good")

words$negative <-  c("bad")

names(words) =  wordName

data_inv_dictonary_sentiment <- dictionary(words)

# Iterate

for (word in pnsSeedPotential){
  
  #### Add a word to old words and calculate AFE (positive and negative)
  
  if (word == "good"){
    pnsSeed_insert <- "good"
  }else{
    pnsSeed_insert <- c(pnsSeed_insert, word) 
  }
  
  #### CONSTUCT DICTIONARY
  
  if (c < length(positive_nw_iter)){
   words$positive <- c(words$positive, word) 
  }else{
   words$negative <- c(words$negative, word)  
  }
  
  #### Construct new dictionary 
  
  data_inv_dictonary_sentiment <- dictionary(words)
  
  
  #### PREDICTION MODEL  
  
  model_lss <- textmodel_lss(dfmt_sent, as.seedwords(data_inv_dictonary_sentiment), k = 300, 
                             auto_weight = TRUE, include_data = TRUE, cache = FALSE) 

  dfm_grouped <- dfm_group(model_lss$data) #reconstruct org. paragraph
  
  dat_tst <- docvars(dfm_grouped) 

  dat_tst$fit <- predict(model_lss, newdata = dfm_grouped, min_n =2, rescale = FALSE)

  dat_tst$text <- as.character(sob) # Get text from corpus for comparability

  dat_tst <- dat_tst %>% mutate(category = case_when(fit >= 0.0001 ~ "positive",
                        fit <= -0.0001  ~ "negative",
                        fit < 0.0001 & fit > -0.0001  ~ "neutral"))
    
  
  #### CALCULATE AFE 
  
  fcm_stat_afe <- vectorCoOccurence(dat_tst) 
  
  
  #### Fill Array
  
  arrayResults[c,1] = word
    
  arrayResults[c,2] = format(fcm_stat_afe, scientific = FALSE)
  
  c = c + 1
  
}
```


```{r SeedwordsInspection, echo=FALSE}


suR <- arrayResults %>%  as_tibble() %>% mutate(`AFE diff` = as.numeric(`AFE`)) %>% 
  mutate(afe_change = (`AFE diff`-lag(`AFE diff`))/ lag(`AFE diff`)) %>%  
  mutate(colHelp = ifelse(afe_change>0, 1, 0))

# lock in factor level order

suR$`+ Seedword` <- factor(suR$`+ Seedword`, levels = suR$`+ Seedword`)

suR %>% ggplot(aes(y= afe_change,x= `+ Seedword`, fill = as.factor(colHelp))) + geom_col() +  
  theme_ipsum(grid="XY", base_family  = "Roboto Condensed", plot_title_size = 12) + 
    theme(axis.text.x = element_text(angle = 90)) + scale_fill_manual(values=c("lightblue", "grey")) +
  theme(legend.position="none")

```


Final seed word selection and model training. Inspection of AFE enhancing seedwords. 

```{r finalseeds, echo=FALSE}

# Inspect the problematic seed words - AFE based 

AFE_problematic_df <- suR %>%  filter(afe_change  > 0.000)

AFE_problematic_df$`+ Seedword` %>%  as_tibble() 

# Keywords in context kwic 

# for (element in 1:length(AFE_problematic_df$`+ Seedword`)){
# 
# kw_main_txt <- kwic(sob, pattern = as.character(AFE_problematic_df[[element,1]]), window = 10)
# 
# head(kw_main_txt, 20) %>%  View()  
# 
# #invisible(readline(prompt="Press [enter] to continue"))
# 
# }

```

# Final selection

```{r finalseedsMerge, echo=FALSE}

AFE_red_df <- suR %>% filter(afe_change  <= 0.000)

seedwords <- AFE_red_df$`+ Seedword`

# Sort for positive and negative words 

seedAFE <- seedwords %>% as_tibble()

# Print

seedAFE

# Delete rows with unclear or possible unclear potential seedwords 

SeedwordsAFEbased <- seedAFE %>% filter(!value %in% c("call","right", "put", "love", "positive")) 

# Add seedword which increased AFE after thorough inspection of KWIC 

postiveAFESeed <- SeedwordsAFEbased[1:7,]

negativeAFESeed <- anti_join(SeedwordsAFEbased,postiveAFESeed, by = 'value')



#Construct dictionary 

words$positive <-  as.character(postiveAFESeed$value)

words$negative <-  as.character(negativeAFESeed$value)

data_afe_dictonary_sentiment <- quanteda::dictionary(words)

data_afe_dictonary_sentiment


```



```{r Predictor, echo=FALSE}

# train model 

model_afe_lss <- textmodel_lss(dfmt_sent, as.seedwords(data_afe_dictonary_sentiment), k = 300, 
                             auto_weight = TRUE, include_data = TRUE, cache = TRUE) 


termP <- head(coef(model_afe_lss), 8) # most positive words

termN <- tail(coef(model_afe_lss), 8) # most negative words

comb <- Combiner(termP, termN)


# Text plot to see the polarity score 

textplot_terms(model_afe_lss, highlighted = names(comb))

print_color("Seedword selection process finalized", 'blue')

```




```{r batchEval, echo=FALSE, fig.width=14, fig.height=12}

# Helper functions 

`%ni%` <- Negate(`%in%`)

# Defined tickers for inspection 

tickers <- c("TLSS", "EEENF","UAPC","PASO","OZSC","ILUS", "BBRW", "RXMD","DECN","HCMC")

endCutOfDate = "2023-01-01"

# Define timeline of interest

dayIntervals <- "1 week"  # daily, weekly, monthly, quarterly 

# Save data points 

arrayForParameters <- array(dim = c(length(tickers),12))

colnames(arrayForParameters) <- c("name", 
                                  "datapoints", 
                                  "BICDiffRatio", 
                                  "lag", 
                                  "Granger Cause sentiment - course",
                                  "p value 1",
                                  "Granger Cause course - sentiment",
                                  "p value 2", 
                                  "BICDiffRatio 2", 
                                  "ADF", 
                                  "lag_aic",
                                  "lag_hq")

# Data load and execution 

count = 1 # for array



print_color("Iteration started", 'green')



for (tick in tickers){
  
  print(paste("Ticker symbol: ", tick))
  
  arrayForParameters[count,1] <- tick 
  
  # Prepare for user data
  
  string_path <- paste(basepath, tick,"/" ,sep="")
  
  # Get stock data
  
  string_path_tickers <- paste(tickerBasePath, tick,".csv" ,sep="")
  
  ticker_df_day <- read.csv(string_path_tickers)

  # Define strings 
  
  comments_general <- paste(string_path, "general_comments_score_",tick,"_added.csv" ,sep="")
  
  comments_assocaited <- paste(string_path, "associated_comments_score_",tick,"_added.csv" ,sep="")
  
  submissions_body <- paste(string_path, "submissions_",tick,"_body.csv" ,sep="")
    
  submissions_headline <- paste(string_path, "submissions_",tick,"_title.csv" ,sep="")
  
  discord_general <- paste(string_path, "discord_",tick,"_body.csv" ,sep="")

  # Load ticker information
  
  comments_general_df <- read.csv(comments_general)
  
  comments_assocaited_df <- read.csv(comments_assocaited)
  
  submissions_body_df <- read.csv(submissions_body)
  
  submissions_headline_df <- read.csv(submissions_headline)
  
  discord_general_df <- read.csv(discord_general)
  
  ################################################ Prepare data for prediction model 
  
  # Append all comments into one data frame (general ,associated and discord)
  
  dsc_general_df <- discord_general_df %>% dplyr::rename(ID = AuthorID) %>% 
    mutate(Score=0, parentID="Null", ID.prefix = "Null", link_id_body = "Null")
  
  # Check if structure like supposed to if not transform columns to be equally structured 
  
  if ("ID.prefix" %ni% colnames(comments_general_df)) {
  comments_general_df <- comments_general_df %>% 
    separate(col=link_id,sep="_",into = c("ID.prefix","link_id_body"))
  }
  
  if ("ID.prefix" %ni% colnames(comments_assocaited_df))  {
  comments_assocaited_df <- comments_assocaited_df %>% 
    separate(col=link_id,sep="_",into = c("ID.prefix","link_id_body"))
  }
  
  # Combine into one data frame 
  
  bind_cmts_it <- rbind(comments_general_df,comments_assocaited_df,dsc_general_df) %>% 
    mutate(DateX = as.Date(Date)) %>% dplyr::select(-Date) %>% dplyr::rename(Date = DateX) %>% distinct()
  
  # Create a corpus and exclude training data 
  
  corp_it <- corpus(bind_cmts_it, text_field = "Body")
  
  sob_it <- corpus_subset(corp_it, Date >= as.Date(cutofDateTraining) & Date <= as.Date(endCutOfDate))  #comp later cut 
  
  toks_sent_it <- sob_it %>% 
    corpus_reshape("sentence") %>% 
    tokens(remove_punct = TRUE) %>% 
    tokens_remove(stopwords("en"), padding = TRUE)
  
  dfmt_sent_it <- toks_sent_it %>% 
    dfm(remove_padding = TRUE) %>%
    dfm_select("^\\p{L}+$", valuetype = "regex", min_nchar = 2) %>% 
    dfm_trim(min_termfreq = 2)
  
  ########################################____Prediction___###################################################

  dfmt_tst_it <- dfm_group(dfmt_sent_it)   

  dat_tst_it <- docvars(dfmt_tst_it) 

  
  # Prediction
  
  print_color(paste("Predicting: ", tick), 'blue')

  dat_tst_it$fit <- predict(model_afe_lss, newdata = dfmt_tst_it, min_n=1, rescale = FALSE)  
  
  arrayForParameters[count,2] <- length(dat_tst_it$ID) 
  
  
  

  
  
  # Defines time span based on sentiment data available in time frame 
  
  print_color("Define timeframe and cut dates based on sentiment data available", 'blue')

  # Define start dates
  
  s_date <- min(dat_tst_it$Date)
  
  if (s_date < cutofDateTraining) {s_date = cutofDateTraining}
  
  # Define end dates
  
  e_date <- max(dat_tst_it$Date)
  
  dat_tst_it <- dat_tst_it %>% filter(Date <= e_date)
  
  print(paste("reported range: start: ", s_date, ", end: ", e_date))
  
  dat_join <- dat_tst_it %>%  dplyr::mutate(date = Date) %>%  dplyr::select(-Date)
  
  # cut Ticker
  
  ticker_df_day <- ticker_df_day %>%  
    filter(Date >= as.Date(s_date) & Date <= as.Date(e_date)) %>% 
    filter(Close != 'null' & Adj.Close!= 'null' ) 
  
  
  
  
  
  
  
  
  ########################################____Weight___#######################################################
  
  # Weighting and scores
  
  # Set score to 1 which indicated no down vote for all the comments from Discord
 
  print_color("Weight scores and remove NA's and 0's ", 'blue')
   
  dat_join <- dat_join  %>% dplyr::mutate(Score = replace_na(Score,0)) %>%  
    dplyr::mutate(fit = replace_na(fit, 0)) %>% dplyr::mutate(Score=ifelse(Score==0,0.5,Score))
  
  # Exclude or dampen scores when down voted to much 
  
  dat_tst_it_weight <- dat_join %>% dplyr::mutate(fit = fit * 1000)  %>% 
    dplyr::mutate(multiplierFromScore  =  2/(2 + 0.05*exp(1)^(-0.9*Score)))   

  
  # Days for summarizing, due to spare trading and comments needed 
  
  print_color(paste(" Summarized sentiment to: ", dayIntervals, " day intervals"), 'blue')
  
  # Group by defined day intervals throughout the N years of available data and sum sentiment 
  
  mean_df <- dat_tst_it_weight %>% 
    timetk::summarise_by_time(fit = sum((fit * (multiplierFromScore))),
    .date_var = date, .by=dayIntervals, dP = n()) %>% drop_na() 
  
  data_Points_Obtain_TS <- zoo(mean_df$dP, mean_df$date)
  
  # Creates an object for predicted values over time
  
  dat_sm_it_ts <- zoo(mean_df$fit, as.Date(mean_df$date, format="%Y-%m-%d"))
  
  mean_ts <- dat_sm_it_ts
  

  
  
  
  
  ### FINANCE DATA
  
  #############################################################################################
  
  # Simple adjusted Closing day from yahoo finance 
  
  ticker_df_day$closePerc <- ticker_df_day$Adj.Close 
  
  ticker_df_day <- ticker_df_day %>% 
    mutate(Date = as.Date(Date)) %>% 
    filter(Date >= s_date & Date <= e_date) %>%  
    as_tibble() %>%  
    dplyr::mutate(across(where(is.character), as.double))
  
    # Compute daily changes
    
    # Close gaps 
    
    tikC <- ticker_df_day %>% 
      read.zoo() %>% 
      as.ts() %>% 
      na.locf() %>% 
      fortify.zoo(name = "Date") %>% 
      transform(Date = as.Date(Date))
    
    # Calculate day interval 
    
    tikTibb <- tikC %>% as_tibble()
    
    # Group by interval
    grpTicker_df_dayrange <- tikTibb %>% 
    timetk::summarise_by_time(.date_var =Date, .by=dayIntervals,
                              adjClose = last(Adj.Close)) %>% drop_na() 
    
    n <- nrow(grpTicker_df_dayrange)
    
    # Get closing % differences per day interval 
     
    grpTicker_df_dayrange <- grpTicker_df_dayrange %>%  filter(Date >= s_date & Date <= e_date)
    
    # TS 
  
    ticker_df_day_ts <- zoo(grpTicker_df_dayrange$adjClose, as.Date(grpTicker_df_dayrange$Date, format="%Y-%m-%d"))
    
    ticker_df_day_ts <- PerformanceAnalytics::CalculateReturns(ticker_df_day_ts) # calc returns
    

  
  ### Comparison plots 
  
  #############################################################################################
  
  # Bring for visualization to same units area - normalize 
  
  tsb_app <- merge.zoo(ticker_df_day_ts, mean_ts)  
  
  tsb_app <- na_replace(tsb_app, fill=0)
  
  ### Stationarity tests
    
  #############################################################################################  
    
  difference_ned_Fc <- forecast::ndiffs(tsb_app[,1])  
  
  difference_ned_Sent <- forecast::ndiffs(tsb_app[,2])
  
  print_color(paste(" Diff required course: ", difference_ned_Fc, 
                    ", Diff required sent: ", difference_ned_Sent), 'blue')
  
  if (difference_ned_Fc > 0){
    tsb_app_c_st <- diff(tsb_app[,1], differences = difference_ned_Fc)
  } else {tsb_app_c_st <- tsb_app[,1]}
  
  if (difference_ned_Sent > 0){
    tsb_app_s_st <- diff(tsb_app[,2], differences = difference_ned_Sent)
  } else {tsb_app_s_st <- tsb_app[,2]}
  
  cADF<- tseries::adf.test(tsb_app_c_st)
  sADF <- tseries::adf.test(tsb_app_s_st)
  
  # ADF test

  print(cADF)
  print(sADF)
  
  arrayForParameters[count,10] <- paste("course: ",cADF$p.value, " sentiment: ", sADF$p.value)
  
  # Sentiment

  p <- ts_ggplot (tsb_app_s_st) +  
  theme_ipsum(grid="XY", base_family  = "Roboto Condensed", plot_title_size = 18) + 
  theme(axis.text.x = element_text(angle = 90)) + 
  ggtitle(paste(tick, "Sentiment change")) + scale_fill_discrete(name = "Plots") + 
  scale_color_manual(values = c("lightblue", "orange")) + 
  stat_peaks(geom = "text", colour = "blue", angle = 90, 
             hjust = -0.1, x.label.fmt = "%d.%m", 
             span = 5) +
  stat_peaks(geom = "rug", colour = "blue", sides = "b") +
  expand_limits(y = max(tsb_app_s_st) + max(tsb_app_s_st)*0.30+ min(tsb_app_s_st)*-0.30)

  # Data points  
  
  p2 <- ts_ggplot (data_Points_Obtain_TS)+  
  theme_ipsum(grid="XY", base_family  = "Roboto Condensed", plot_title_size = 18) + 
  theme(axis.text.x = element_text(angle = 90)) + 
  ggtitle(paste(tick, "Data points (base for sentiment estimation)")) + scale_fill_discrete(name = "Plots") + 
  scale_color_manual(values = c("lightblue", "orange")) + 
  stat_peaks(geom = "text", colour = "black", angle = 90, hjust = -0.1, x.label.fmt = "%d.%m", span = 5) +
  stat_peaks(geom = "rug", colour = "black", sides = "b") 
  
  # Course 

  p3 <- ts_ggplot (tsb_app_c_st) +  
  theme_ipsum(grid="XY", base_family  = "Roboto Condensed", plot_title_size = 18) + 
  theme(axis.text.x = element_text(angle = 90)) + 
  ggtitle(paste(tick, "closing courses")) + scale_fill_discrete(name = "Plots") + 
  scale_color_manual(values = c("lightblue", "orange")) + 
  stat_peaks(geom = "text", colour = "blue", angle = 90, 
             hjust = -0.1, x.label.fmt = "%d.%m", span = 5) +
  stat_peaks(geom = "rug", colour = "blue", sides = "b") +
  expand_limits(y = max(tsb_app_c_st) + max(tsb_app_c_st)*0.30+ min(tsb_app_c_st)*-0.30)
  
  pa <- plot_grid(p, p3, align="v", ncol = 1)
  
  print(pa)
  
  ##########################__Granger__################################################################
  
  print("Granger correlation tests")
  print("closing course with sentiment")
  
  # First dependent second independent 
  
  # SPLIT TIME SERIES 
  
  tb_sent <- tsb_app_s_st
  
  tb_course <- tsb_app_c_st
  
  tsma <- merge(tb_sent,tb_course)
  
  # insert missing high freq data if required #################################
  
  if (dayIntervals == "1 day") {
      
    ts_zoo_complete <- zoo(tsma, seq(from = start(tsma), to = end(tsma), by = "day"))
    
    coredata(ts_zoo_complete)[is.na(ts_zoo_complete)] <- 0
    
    tsma <- ts_zoo_complete
  }

  tsma <- tsma[-1,]
  
  
  # Lag selection ##############################################################
  
  max_lag <- 10  
  
  aic_var_lags <- VARselect(tsma, lag.max = max_lag, type = "both")
  
  optimal_lag_aic <- aic_var_lags$selection[1]
  optimal_lag_hq <- aic_var_lags$selection[2]
  
  arrayForParameters[count,11] <- optimal_lag_aic
  arrayForParameters[count,12] <- optimal_lag_hq
  
  # Cross-correlation
  ccf_result <- ccf(tsma[,1], tsma[,2], lag.max = max_lag, plot = FALSE)  #
  
  # Lag with the highest cross-correlation
  optimal_lag_ccf <- which.max(abs(ccf_result$acf))

  # Decide between lags 
  
  # Majority vote for optimal lag
  lag_counts <- table(c(optimal_lag_aic, optimal_lag_hq, optimal_lag_ccf))
  optimal_lag <- as.numeric(names(lag_counts[lag_counts >= 2]))
  
  # Default BIC 
  if (length(optimal_lag) == 0) {
    optimal_lag <- optimal_lag_hq
  }
    
  ##############################################################################
  
  # Split 
  
  tb_sent <- tsma[,1]
  
  tb_course <- tsma[,2]
  
  # Granger function 
  
  tstRes <- VLTimeCausality::VLGrangerFunc(X = tb_sent, Y= tb_course, maxLag = optimal_lag, autoLagflag=FALSE) 
  
  arrayForParameters[count,3] <- tstRes$BICDiffRatio 
  
  arrayForParameters[count,4] <- tstRes$maxLag
  
  arrayForParameters[count,5] <- tstRes$XgCsY_ftest
  
  arrayForParameters[count,6] <- tstRes$p.val
  
  
  tstRes <- VLTimeCausality::VLGrangerFunc(X = tb_course, Y= tb_sent, maxLag = optimal_lag, autoLagflag=FALSE)
  
  arrayForParameters[count,7] <- tstRes$XgCsY_ftest

  arrayForParameters[count,8] <- tstRes$p.val
  
  arrayForParameters[count,9] <- tstRes$BICDiffRatio
  
  print("----------------------------------------------------------------------")
  
  count = count + 1
  
}

arr <- arrayForParameters %>%  as_tibble()

arr

# for every changed duration iteration


pth<- paste("C:/Users/elias/OneDrive/Desktop/Meme_Thesis/7 Results H1/new_LSS_",dayIntervals,".xlsx", sep="")

openxlsx::write.xlsx(arr,pth)


```

