---
title: "LSS"
author: "Elias Mayer"
date: "2022-10-29"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r libs, echo=FALSE, warning=FALSE, include=FALSE}

library("ggpmisc")

library("tidyverse")
library("tidytext")
library("tokenizers")
library("tidymodels")

library("purrr")


library ("plyr")
library("stopwords")
library("readr")
library("quanteda")
library("quanteda.textstats")
library("quanteda.textplots")

library("quanteda")
library("LSX") 

# For stock symbols
library("TTR")

# English lexicons not sentiment
library("qdapDictionaries")

# Head tail
library("psych")

# imputes if needed
library("imputeTS")
library("tseries")
library("forecast")
library("urca")

# font_import() this is required for chosen clean theme 

#library(extrafont) 

#loadfonts(device = "win")

#extrafont::font_import()

#detach("package:hrbrthemes", unload=TRUE)

library("ragg")
library("hrbrthemes")

hrbrthemes::import_roboto_condensed()

library("rlist")
library("DescTools")
library("newsmap")

library("tsibble")

# for ts conversions
library("tsbox")

# variable lag granger
library("VLTimeCausality")

# TS and plotting
library("zoo")
library("cowplot")

```

##  Load data and preperation

```{r TrainLSS, echo=FALSE}


# Adjust path to Folder Struct 

basePathC <- 'C:/Users/elias/OneDrive/Desktop/Meme_Thesis/3 Data/Cleaned_relevant_data'

# Paths for Ticker data

basepath <- "C:/Users/elias/OneDrive/Desktop/Meme_Thesis/3 Data/Cleaned_relevant_data/Ticker specific/"
  
tickerBasePath <- "C:/Users/elias/OneDrive/Desktop/Meme_Thesis/3 Data/Stock_data_WF/"


# Combiner for graph 

# Source: https://stackoverflow.com/questions/29864318/combination-of-named-vectors 

Combiner <- function(vec1, vec2, vecOut = TRUE) {
  temp <- unique(rbind(data.frame(as.table(vec1)),
                       data.frame(as.table(vec2))))
  if (isTRUE(vecOut)) setNames(temp$Freq, temp$Var1)
  else temp
}

#Train the LSS with the whole corpus available (training) of all OTC tickers to pick up as much language wise as possible 

whole_gen_cmts_language_df <-  read.csv(paste(basePathC, "/reddit_comments/general_comments.csv", sep = ""))

whole_ass_cmts_language_df <-  read.csv(paste(basePathC, "/reddit_comments/associated_comments.csv", sep = ""))

whole_dis_cmts_language_df <-  read.csv(paste(basePathC, "/discord_comments/discord_comments.csv", sep = ""))


# Time split sufficient 

whole_gen_cmts_language_df_cl <- whole_gen_cmts_language_df %>% 
  separate(col=link_id,sep="_",into = c("ID.prefix","link_id_body"))

whole_dis_cmts_language_df_cl <- whole_dis_cmts_language_df %>% 
  dplyr::rename(ID = AuthorID) %>% mutate(Score=1, parentID="Null", ID.prefix = "Null", link_id_body = "Null")

# Append all comments into one data frame (general ,associated and discord)

bind_cmts <- rbind(whole_gen_cmts_language_df_cl,whole_ass_cmts_language_df,whole_dis_cmts_language_df_cl) %>% 
  mutate(DateX = as.Date(Date)) %>% dplyr::select(-Date) %>% dplyr::rename(Date = DateX) %>% distinct()


# Create a corpus combined

corp <- corpus(bind_cmts, text_field = "Body")

summary(docvars(corp))

cutofDateTraining <- "2020-03-01"

sob <- corpus_subset(corp, Date < as.Date(cutofDateTraining))  # Sub-setting for training data 

sum_tok <- summary(sob, n = Inf)

# Preperation, Source: https://tutorials.quanteda.io/machine-learning/lss/

ggplot(data = sum_tok, aes(x = Date, y = Sentences)) +
  geom_area( fill="#69b3a2", alpha=0.4) +
  geom_line(color="#69b3a2", size=0.02) +
  coord_cartesian(ylim=c(0,50))  +  
  theme_ipsum(grid="Y", base_family  = "Roboto Condensed", plot_title_size = 12) 

toks_sent <- sob %>% 
  corpus_reshape("sentence") %>% 
  tokens(remove_punct = TRUE) %>% 
  tokens_remove(stopwords("en"), padding = TRUE)

dfmt_sent <- toks_sent %>% 
  dfm(remove_padding = TRUE) %>%
  dfm_select("^\\p{L}+$", valuetype = "regex", min_nchar = 2) %>% 
  dfm_trim(min_termfreq = 10)

textplot_wordcloud(dfmt_sent, min_count = 6, random_order = FALSE, rotation = 0.25, max_words = 100)


```



```{r SeedwordsAFE, echo=FALSE}

# Function which filters and gives the AFE stat for given data 

vectorCoOccurence <- function(DATA) {

    dat_tst <- DATA
    
    corb <- quanteda::corpus(dat_tst, text_field = "text") %>% tokens(remove_punct = TRUE)  %>% 
      dfm(remove_padding = TRUE) 
    
    dfm_labels <- dat_tst$category %>% tokens(remove_punct = TRUE) %>%  dfm(remove_padding = TRUE) 
    
    # create co-occurrence vectors
    afe_stat <- newsmap::afe(corb, dfm_labels, smooth = 1)

    return(afe_stat)
}

```

# Classify for AFE

```{r SeedwordsAFEAddition, echo=FALSE, warning=FALSE}

# Potential seed words from frequency selected after human double check

positive_nw_iter <- c( "good", "bought","right","pump",
                       "nice","holding","great", "gain", "buy", 
                       "upvote", "strong", "super", "call", 
                       "moon", "love","positive","win")

negative_nw_iter <- c( "bad","short", "sold","wrong","dump",
                       "shit","selling","concerns", "drop","sell", 
                       "downvote", "spam", "fuck", "scam",
                       "put","panic","negative", "loss")

pnsSeedPotential<- c(positive_nw_iter,negative_nw_iter)

# Array for results 

arrayResults <- array(dim = c(length(pnsSeedPotential),2)) 

colnames(arrayResults) <- c("+ Seedword", "AFE")

c = 1

# Start words

wordName = c("positive","negative")

words <- list()

words$positive <-  c("good")

words$negative <-  c("bad")

names(words) =  wordName

data_inv_dictonary_sentiment <- dictionary(words)

# Iterate

for (word in pnsSeedPotential){
  
  #### Add a word to old words and calculate AFE (positive and negative)
  
  if (word == "good"){
    pnsSeed_insert <- "good"
  }else{
    pnsSeed_insert <- c(pnsSeed_insert, word) 
  }
  
  #### CONSTUCT DICTIONARY
  
  if (c < length(positive_nw_iter)){
   words$positive <- c(words$positive, word) 
  }else{
   words$negative <- c(words$negative, word)  
  }
  
  #### Construct new dictionary 
  
  data_inv_dictonary_sentiment <- dictionary(words)
  
  
  #### PREDICTION MODEL  
  
  model_lss <- textmodel_lss(dfmt_sent, as.seedwords(data_inv_dictonary_sentiment), k = 300, 
                             auto_weight = TRUE, include_data = TRUE, cache = FALSE) 

  dfm_grouped <- dfm_group(model_lss$data) #reconstruct org. paragraph
  
  dat_tst <- docvars(dfm_grouped) 

  dat_tst$fit <- predict(model_lss, newdata = dfm_grouped, min_n =2, rescale = FALSE)

  dat_tst$text <- as.character(sob) # Get text from corpus for comparability

  dat_tst <- dat_tst %>% mutate(category = case_when(fit >= 0.0001 ~ "positive",
                        fit <= -0.0001  ~ "negative",
                        fit < 0.0001 & fit > -0.0001  ~ "neutral"))
    
  
  #### CALCULATE AFE 
  
  fcm_stat_afe <- vectorCoOccurence(dat_tst) 
  
  
  #### Fill Array
  
  arrayResults[c,1] = word
    
  arrayResults[c,2] = format(fcm_stat_afe, scientific = FALSE)
  
  c = c + 1
  
}
```


```{r SeedwordsInspection, echo=FALSE}


suR <- arrayResults %>%  as_tibble() %>% mutate(`AFE diff` = as.numeric(`AFE`)) %>% 
  mutate(afe_change = (`AFE diff`-lag(`AFE diff`))/ lag(`AFE diff`)) %>%  
  mutate(colHelp = ifelse(afe_change>0, 1, 0))

# lock in factor level order

suR$`+ Seedword` <- factor(suR$`+ Seedword`, levels = suR$`+ Seedword`)

suR %>% ggplot(aes(y= afe_change,x= `+ Seedword`, fill = as.factor(colHelp))) + geom_col() +  
  theme_ipsum(grid="XY", base_family  = "Roboto Condensed", plot_title_size = 12) + 
    theme(axis.text.x = element_text(angle = 90)) + scale_fill_manual(values=c("lightblue", "grey")) +
  theme(legend.position="none")

```


Final seed word selection and model training. Inspection of AFE enhancing seedwords. 

```{r finalseeds, echo=FALSE}

# Inspect the problematic seed words - AFE based 

AFE_problematic_df <- suR %>%  filter(afe_change  > 0.000)

AFE_problematic_df$`+ Seedword` %>%  as_tibble() 

# Keywords in context kwic 

# for (element in 1:length(AFE_problematic_df$`+ Seedword`)){
# 
# kw_main_txt <- kwic(sob, pattern = as.character(AFE_problematic_df[[element,1]]), window = 10)
# 
# head(kw_main_txt, 20) %>%  View()  
# 
# #invisible(readline(prompt="Press [enter] to continue"))
# 
# }

```

# Final selection

```{r finalseedsMerge, echo=FALSE}

AFE_red_df <- suR %>% filter(afe_change  <= 0.000)

seedwords <- AFE_red_df$`+ Seedword`

# Sort for positive and negative words 

seedAFE <- seedwords %>% as_tibble()

# Print

seedAFE

# Delete rows with unclear or possible unclear potential seedwords 

SeedwordsAFEbased <- seedAFE %>% filter(!value %in% c("call","right", "put", "love", "positive")) 

# Add seedword which increased AFE after thorough inspection of KWIC 

postiveAFESeed <- SeedwordsAFEbased[1:7,]

negativeAFESeed <- anti_join(SeedwordsAFEbased,postiveAFESeed, by = 'value')



#Construct dictionary 

words$positive <-  as.character(postiveAFESeed$value)

words$negative <-  as.character(negativeAFESeed$value)

data_afe_dictonary_sentiment <- quanteda::dictionary(words)

data_afe_dictonary_sentiment


```



```{r Predictor, echo=FALSE}

# train model 

model_afe_lss <- textmodel_lss(dfmt_sent, as.seedwords(data_afe_dictonary_sentiment), k = 300, 
                             auto_weight = TRUE, include_data = TRUE, cache = TRUE) 


termP <- head(coef(model_afe_lss), 8) # most positive words

termN <- tail(coef(model_afe_lss), 8) # most negative words

comb <- Combiner(termP, termN)


# Text plot to see the polarity score 

textplot_terms(model_afe_lss, highlighted = names(comb))


```




```{r batchEval, echo=FALSE, fig.width=14, fig.height=12}

# Helper functions 

`%ni%` <- Negate(`%in%`)

# Defined tickers for inspection 

tickers <- c("TLSS", "EEENF","UAPC","PASO","OZSC","ILUS", "BBRW", "RXMD","DECN","HCMC")

endCutOfDate = "2023-01-01"

# Save data points 

arrayForParameters <- array(dim = c(length(tickers),9))

colnames(arrayForParameters) <- c("name", "datapoints in ti", "BICDiffRatio", "VAR lag", 
                                  "Granger Cause sentiment - course","p value 1",
                                  "Granger Cause course - sentiment","p value 2", "BICDiffRatio 2")

# Data load and execution 

count = 1 # for array


for (tick in tickers){
  
  print(paste("Ticker symbol: ", tick))
  
  arrayForParameters[count,1] <- tick 
  
  # Prepare for user data
  
  string_path <- paste(basepath, tick,"/" ,sep="")
  
  # Get stock data
  
  string_path_tickers <- paste(tickerBasePath, tick,".csv" ,sep="")
  
  ticker_df_day <- read.csv(string_path_tickers)
  
  ticker_df_day <- ticker_df_day %>%  
    filter(Date >= as.Date(cutofDateTraining) & Date <= as.Date(endCutOfDate)) %>% 
    filter(Close != 'null' & Adj.Close!= 'null' ) 
  
  # Define strings 
  
  comments_general <- paste(string_path, "general_comments_score_",tick,"_added.csv" ,sep="")
  
  comments_assocaited <- paste(string_path, "associated_comments_score_",tick,"_added.csv" ,sep="")
  
  submissions_body <- paste(string_path, "submissions_",tick,"_body.csv" ,sep="")
    
  submissions_headline <- paste(string_path, "submissions_",tick,"_title.csv" ,sep="")
  
  discord_general <- paste(string_path, "discord_",tick,"_body.csv" ,sep="")

  # Load ticker information
  
  comments_general_df <- read.csv(comments_general)
  
  comments_assocaited_df <- read.csv(comments_assocaited)
  
  submissions_body_df <- read.csv(submissions_body)
  
  submissions_headline_df <- read.csv(submissions_headline)
  
  discord_general_df <- read.csv(discord_general)
  
  ################################################ Prepare data for prediction model 
  
  # Append all comments into one data frame (general ,associated and discord)
  
  dsc_general_df <- discord_general_df %>% dplyr::rename(ID = AuthorID) %>% 
    mutate(Score=0, parentID="Null", ID.prefix = "Null", link_id_body = "Null")
  
  # Check if structure like supposed to if not transform columns to be equally structured 
  
  if ("ID.prefix" %ni% colnames(comments_general_df)) {
  comments_general_df <- comments_general_df %>% 
    separate(col=link_id,sep="_",into = c("ID.prefix","link_id_body"))
  }
  
  if ("ID.prefix" %ni% colnames(comments_assocaited_df))  {
  comments_assocaited_df <- comments_assocaited_df %>% 
    separate(col=link_id,sep="_",into = c("ID.prefix","link_id_body"))
  }
  
  # Combine into one data frame 
  
  bind_cmts_it <- rbind(comments_general_df,comments_assocaited_df,dsc_general_df) %>% 
    mutate(DateX = as.Date(Date)) %>% dplyr::select(-Date) %>% dplyr::rename(Date = DateX) %>% distinct()
  
  # Create a corpus and exclude training data 
  
  corp_it <- corpus(bind_cmts_it, text_field = "Body")
  
  sob_it <- corpus_subset(corp_it, Date >= as.Date(cutofDateTraining) & Date <= as.Date(endCutOfDate))  
  
  toks_sent_it <- sob_it %>% 
    corpus_reshape("sentence") %>% 
    tokens(remove_punct = TRUE) %>% 
    tokens_remove(stopwords("en"), padding = TRUE)
  
  dfmt_sent_it <- toks_sent_it %>% 
    dfm(remove_padding = TRUE) %>%
    dfm_select("^\\p{L}+$", valuetype = "regex", min_nchar = 2) %>% 
    dfm_trim(min_termfreq = 5)
  
  ########################################____Prediction___###################################################

  dfmt_tst_it <- dfm_group(dfmt_sent_it)   

  dat_tst_it <- docvars(dfmt_tst_it) 

  
  # Prediction

  dat_tst_it$fit <- predict(model_afe_lss, newdata = dfmt_tst_it, min_n=1, rescale = FALSE)  
  
  arrayForParameters[count,2] <- length(dat_tst_it$ID) 
  
  
  # Defines time span based on sentiment data available in time frame

  s_date <- min(dat_tst_it$Date)
  
 if (s_date < cutofDateTraining) {s_date = cutofDateTraining}
  
  e_date <- max(dat_tst_it$Date)
  
  print(paste("reported range: start: ", s_date, ", end: ", e_date))
  
  dat_join <- dat_tst_it %>%  dplyr::mutate(date = Date) %>%  select(-Date)
  
  ########################################____Weight___#######################################################
  
  # Weighting and scores
  
  # Set score to 1 which indicated no down vote for all the comments from Discord
  
  dat_join <- dat_join  %>% dplyr::mutate(Score = replace_na(Score,0)) %>%  
    dplyr::mutate(fit = replace_na(fit, 0)) %>% 
    dplyr::rename(Date = date) %>% dplyr::mutate(Score=ifelse(Score==0,0.5,Score))
  
  
  # Weight Reddit scores - please mind score weights are not utilized 
  
  # https://www.desmos.com/calculator/uwgcqjafuh?lang=de 

  dat_tst_it_weight <- dat_join %>% dplyr::mutate(fit = fit * 1000)  %>% 
    dplyr::mutate(multiplierFromScore  =  2/(2 + 0.05*exp(1)^(-0.9*Score)))   #exclude or dampen scores when downvoted to much 

  # Sum 
  mean_df <- dat_tst_it_weight %>% dplyr::group_by(Date) %>%  
    dplyr::summarise(fit = sum(fit * multiplierFromScore)) %>% drop_na() 
  
  
  # Combines to zoo ts object 
  
  # Creates an object which counts data points
  
  datPoints <- dat_tst_it %>% tibble() %>% 
    filter(is.na(fit)==FALSE) %>% dplyr::group_by(Date) %>%  
    dplyr::summarise(dP = count(Date))
  
  data_Points_Obtain_TS <- zoo(datPoints$dP$freq, datPoints$Date)
  
  # Creates an object for predicted values over time
  
  dat_sm_it_ts <- zoo(mean_df$fit, mean_df$Date) 
  
  mean_ts <- dat_sm_it_ts
  

  ### FINANCE DATA
  
  #############################################################################################
  
  # Simple adjusted Closing day from yahoo finance 
  
  ticker_df_day$closePerc <- ticker_df_day$Adj.Close 
  
  ticker_df_day <- ticker_df_day %>% 
    mutate(Date = as.Date(Date)) %>% 
    filter(Date >= s_date & Date <= e_date) %>%  
    as_tibble() %>%  
    dplyr::mutate(across(where(is.character), as.double))
  
  # TS 

  ticker_df_day_ts <- zoo(ticker_df_day$closePerc, ticker_df_day$Date) 
  
  ticker_df_day_ts <- na_replace(ticker_df_day_ts, fill=0) # zero course change for NA or weekend days  
  
  
  ### Comparison plots 
  
  #############################################################################################
  
  # Bring for visualization to same units area - normalize 
  
  tsb_app <- merge(ticker_df_day_ts, mean_ts) 
  
  tsb_app <- na_replace(tsb_app, fill=0) 

  # Sentiment

  p <- ts_ggplot (mean_ts) +  
  theme_ipsum(grid="XY", base_family  = "Roboto Condensed", plot_title_size = 12) + 
  theme(axis.text.x = element_text(angle = 90)) + 
  ggtitle(paste(tick, "Sentiment change")) + scale_fill_discrete(name = "Plots") + 
  scale_color_manual(values = c("lightblue", "orange")) + 
  stat_peaks(geom = "text", colour = "blue", angle = 90, hjust = -0.1, x.label.fmt = "%d.%m", span = 21) +
  stat_peaks(geom = "rug", colour = "blue", sides = "b") +
  expand_limits(y = max(mean_ts) + max(mean_ts)*0.30+ min(mean_ts)*-0.30)

  # Data points  
  
  p2 <- ts_ggplot (data_Points_Obtain_TS)+  
  theme_ipsum(grid="XY", base_family  = "Roboto Condensed", plot_title_size = 12) + 
  theme(axis.text.x = element_text(angle = 90)) + 
  ggtitle(paste(tick, "Data points (base for sentiment estimation)")) + scale_fill_discrete(name = "Plots") + 
  scale_color_manual(values = c("lightblue", "orange")) + 
  stat_peaks(geom = "text", colour = "black", angle = 90, hjust = -0.1, x.label.fmt = "%d.%m", span = 21) +
  stat_peaks(geom = "rug", colour = "black", sides = "b") +
  expand_limits(y = max(data_Points_Obtain_TS) + max(data_Points_Obtain_TS)*0.20)
  
  # Course 

  p3 <- ts_ggplot (ticker_df_day_ts) +  
  theme_ipsum(grid="XY", base_family  = "Roboto Condensed", plot_title_size = 12) + 
  theme(axis.text.x = element_text(angle = 90)) + 
  ggtitle(paste(tick, "closing courses")) + scale_fill_discrete(name = "Plots") + 
  scale_color_manual(values = c("lightblue", "orange")) + 
  stat_peaks(geom = "text", colour = "blue", angle = 90, ignore_threshold= max(ticker_df_day_ts)/5,
             hjust = -0.1, x.label.fmt = "%d.%m", span = 21) +
  stat_peaks(geom = "rug", colour = "blue", sides = "b") +
  expand_limits(y = max(ticker_df_day_ts) + max(ticker_df_day_ts)*0.20)
  
  pa <- plot_grid(p, p2, p3, align="v", ncol = 1)
  
  print(pa)
  
  ##########################__Granger__################################################################
  
  print("Granger correlation tests")
  print("closing course with sentiment")
  
  # First dependent second independent 
  
  tsb_app # SPLIT TIME SERIES 
  
  tb_sent <- tsb_app[,2]
  
  tb_course <- tsb_app[,1]
  
  
  
  # Granger function 
  
  tstRes <- VLTimeCausality::VLGrangerFunc(X = tb_sent, Y= tb_course, maxLag = 5, autoLagflag = TRUE)
  
  arrayForParameters[count,3] <- tstRes$BICDiffRatio 
  
  arrayForParameters[count,4] <- tstRes$maxLag
  
  arrayForParameters[count,5] <- tstRes$XgCsY_ftest
  
  arrayForParameters[count,6] <- tstRes$p.val
  
  
  tstRes <- VLTimeCausality::VLGrangerFunc(X = tb_course, Y= tb_sent, maxLag = 5, autoLagflag = TRUE)
  
  arrayForParameters[count,7] <- tstRes$XgCsY_ftest

  arrayForParameters[count,8] <- tstRes$p.val
  
  arrayForParameters[count,9] <- tstRes$BICDiffRatio
  
  print("----------------------------------------------------------------------")
  
  count = count + 1
  
}

arr <- arrayForParameters %>%  as_tibble()

arr

```

